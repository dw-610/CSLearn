{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Using CSLearn to Train an Image Domain Learner\n",
    "\n",
    "This notebook provides a demo that shows how to use the ImageLearningController API to train an image domain learner model on a locally-saved dataset. This model uses a custom CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries - Import and Initialize\n",
    "\n",
    "First, we'll import the API from the `controllers` module. Then we'll initialize the API, telling it that we intend to train a domain learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: comet_ml not installed. Comet ML logging will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 13:35:24.158495: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 13:35:24.182349: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-24 13:35:24.182377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-24 13:35:24.183220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-24 13:35:24.187925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 13:35:24.603663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from cslearn.controllers import ImageLearningController\n",
    "\n",
    "ctrl = ImageLearningController(learner_type='domain_learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the Data Loaders\n",
    "\n",
    "Next we'll use the `create_data_loaders` method to indicate what data we'll be using to train our classifier. We are going to use a locally saved dataset to train the domain learner. This data must be saved as four *memory-mapped* .npy files (i.e., they were created using `numpy.memmap(filename, mode='w+', ...)`) containing the training data, validation data, training labels and validation labels. To be compatible with `create_data_loaders`, the data in each file must have `dtype=np.float32`. The labels should be one-hot encoded.\n",
    "\n",
    "To load the data, we need to pass two dictionaries to `create_data_loaders`: one containing the paths to each data file and one containing the shape of each data array. Examples for some demo data are included below (the demo data is a small subset of the CIFAR10 dataset).\n",
    "\n",
    "`create_data_loaders` creates `tf.data.Dataset` objects under the hood to handle the data that is passed to the model during training. As a consequence, we need to specify the batch size used when creating the data loaders - this will be the minibatch size used during training of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 13:35:25.126489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.154467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.154601: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.156397: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.156549: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.156629: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.203106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.203213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.203271: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 13:35:25.203318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1110 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "paths_dict = {\n",
    "    'train_data_path': 'demo_data/demo_cifar10subset_trn_data.npy',\n",
    "    'train_labels_path': 'demo_data/demo_cifar10subset_trn_labels.npy',\n",
    "    'valid_data_path': 'demo_data/demo_cifar10subset_vld_data.npy',\n",
    "    'valid_labels_path': 'demo_data/demo_cifar10subset_vld_labels.npy',\n",
    "}\n",
    "\n",
    "shapes_dict = {\n",
    "    'train_data_shape': (500, 32, 32, 3),\n",
    "    'train_labels_shape': (500,10),\n",
    "    'valid_data_shape': (100, 32, 32, 3),\n",
    "    'valid_labels_shape': (100,10),\n",
    "}\n",
    "\n",
    "ctrl.create_data_loaders(\n",
    "    dataset='local', \n",
    "    batch_size=16,\n",
    "    paths_dict=paths_dict,\n",
    "    shapes_dict=shapes_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create and Compile the Model(s)\n",
    "\n",
    "The next step is to create the learner that we'll be training. This is done using the `create_learner` method. Note that this method creates multiple models assigned as attributes to our `ctrl` object - for the domain learner model type, it creates `encoder` and `decoder` sub-models and the overall domain learner model that is stored as `model`. You can customize nearly every aspect of the CNN when using `custom_cnn` - we'll use the default parameters here.\n",
    "\n",
    "We set `latent_dim=2` to indicate that the domain we are learning is two-dimensional. `latent_dim` can be any positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl.create_learner(latent_dim=2, architecture='custom_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the leaner, we need to compile it. This is done with the `compile_leaner` method, which takes arguments such as the loss function to be used and metrics to save during training. \n",
    "\n",
    "The domain learner uses a custom loss function, so we do not need to specify the `loss` parameter here. However, we can set the hyperparameters for the loss function `alpha`, `beta`, and `lam` here. See [cite paper] for details on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl.compile_learner(\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    lam=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSLearn comes with a helper method for summarizing the models that you created - simply call `summarize_models` with no inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "                           encoder                          \n",
      "_________________________________________________________________\n",
      " Layer                       Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1                     (None, 32, 32, 3)         0\n",
      " convolution_block           (None, 16, 16, 16)        2432\n",
      " convolution_block_1         (None, 16, 16, 16)        6480\n",
      " convolution_block_2         (None, 8, 8, 32)          4768\n",
      " convolution_block_3         (None, 8, 8, 32)          9376\n",
      " global_average_pooling2d    (None, 32)                0\n",
      " dense                       (None, 2)                 66\n",
      "=================================================================\n",
      "Total params: 23122\n",
      "Trainable params: 22930\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "                           decoder                          \n",
      "_________________________________________________________________\n",
      " Layer                       Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1                     (None, 2048)              6144\n",
      " reshape                     (None, 8, 8, 32)          0\n",
      " deconvolution_block         (None, 8, 8, 32)          9376\n",
      " deconvolution_block_1       (None, 16, 16, 16)        4688\n",
      " deconvolution_block_2       (None, 16, 16, 16)        6480\n",
      " deconvolution_block_3       (None, 32, 32, 3)         2367\n",
      "=================================================================\n",
      "Total params: 29055\n",
      "Trainable params: 28921\n",
      "Non-trainable params: 134\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "                           domain_learner_model                          \n",
      "_________________________________________________________________\n",
      " Layer                       Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder                     (None, 2)                 23122\n",
      " decoder                     (None, 32, 32, 3)         29055\n",
      " euclidean_distance_layer    (None, 10)                20\n",
      " soft_gauss_sim_prediction_layer(None, 10)                0\n",
      "=================================================================\n",
      "Total params: 52197\n",
      "Trainable params: 51851\n",
      "Non-trainable params: 351\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctrl.summarize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the Learner\n",
    "\n",
    "After we have created and compiled the leaner, we use the `train_leaner` method to initiate training. This method takes various parameters related to the training algorithm. We'll only specify the number of epochs and indicate that we would like a verbose output during training.\n",
    "\n",
    "The default verbosity of 1 can get a bit messy for the domain learner, so we'll use `verbose=2`, which provides an output that is a bit more suppressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch 1/10 (warmup)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 13:35:27.593207: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-24 13:35:27.642651: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-24 13:35:27.773697: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-24 13:35:29.166351: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f395041da40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-24 13:35:29.166376: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-02-24 13:35:29.170121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708803329.223417  296189 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 6s - loss: 3.5309 - wl_r: 0.7580 - wl_c: 2.7476 - wl_d: 0.0253 - l_r: 0.7580 - l_c: 2.7476 - l_d: 0.2528 - accuracy: 0.1250 - val_loss: 2.5888 - val_l_r: 0.3368 - val_l_c: 2.2265 - val_l_d: 0.2552 - val_accuracy: 0.1600 - 6s/epoch - 184ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2/2\n",
      "32/32 - 1s - loss: 2.8211 - wl_r: 0.7044 - wl_c: 2.1150 - wl_d: 0.0017 - l_r: 0.7044 - l_c: 2.1150 - l_d: 0.0168 - accuracy: 0.1602 - val_loss: 3.2984 - val_l_r: 1.1493 - val_l_c: 2.1482 - val_l_d: 0.0082 - val_accuracy: 0.1600 - 798ms/epoch - 25ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "Epoch 3/3\n",
      "32/32 - 1s - loss: 2.9627 - wl_r: 0.7290 - wl_c: 2.2321 - wl_d: 0.0016 - l_r: 0.7290 - l_c: 2.2321 - l_d: 0.0164 - accuracy: 0.2070 - val_loss: 4.2107 - val_l_r: 1.9458 - val_l_c: 2.2648 - val_l_d: 6.4900e-04 - val_accuracy: 0.1200 - 850ms/epoch - 27ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Epoch 4/4\n",
      "32/32 - 1s - loss: 2.7889 - wl_r: 0.5551 - wl_c: 2.2304 - wl_d: 0.0034 - l_r: 0.5551 - l_c: 2.2304 - l_d: 0.0343 - accuracy: 0.2109 - val_loss: 4.2540 - val_l_r: 1.9399 - val_l_c: 2.3139 - val_l_d: 0.0022 - val_accuracy: 0.1000 - 828ms/epoch - 26ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Epoch 5/5\n",
      "32/32 - 1s - loss: 2.7047 - wl_r: 0.4961 - wl_c: 2.2015 - wl_d: 0.0072 - l_r: 0.4961 - l_c: 2.2015 - l_d: 0.0719 - accuracy: 0.1973 - val_loss: 3.9616 - val_l_r: 1.6180 - val_l_c: 2.3429 - val_l_d: 0.0058 - val_accuracy: 0.0800 - 815ms/epoch - 25ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Epoch 6/6\n",
      "32/32 - 1s - loss: 2.6973 - wl_r: 0.4611 - wl_c: 2.2218 - wl_d: 0.0144 - l_r: 0.4611 - l_c: 2.2218 - l_d: 0.1439 - accuracy: 0.2188 - val_loss: 3.5095 - val_l_r: 1.1519 - val_l_c: 2.3554 - val_l_d: 0.0226 - val_accuracy: 0.0600 - 820ms/epoch - 26ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "Epoch 7/7\n",
      "32/32 - 1s - loss: 2.4567 - wl_r: 0.3901 - wl_c: 2.0401 - wl_d: 0.0264 - l_r: 0.3901 - l_c: 2.0401 - l_d: 0.2645 - accuracy: 0.2168 - val_loss: 3.3533 - val_l_r: 0.9112 - val_l_c: 2.4394 - val_l_d: 0.0273 - val_accuracy: 0.0900 - 818ms/epoch - 26ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Epoch 8/8\n",
      "32/32 - 1s - loss: 2.3965 - wl_r: 0.4086 - wl_c: 1.9716 - wl_d: 0.0163 - l_r: 0.4086 - l_c: 1.9716 - l_d: 0.1626 - accuracy: 0.2285 - val_loss: 3.1569 - val_l_r: 0.6721 - val_l_c: 2.4815 - val_l_d: 0.0322 - val_accuracy: 0.1100 - 811ms/epoch - 25ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "Epoch 9/9\n",
      "32/32 - 1s - loss: 2.5146 - wl_r: 0.3701 - wl_c: 2.1112 - wl_d: 0.0333 - l_r: 0.3701 - l_c: 2.1112 - l_d: 0.3330 - accuracy: 0.2441 - val_loss: 3.1432 - val_l_r: 0.5884 - val_l_c: 2.5501 - val_l_d: 0.0467 - val_accuracy: 0.1000 - 822ms/epoch - 26ms/step\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "Epoch 10/10\n",
      "32/32 - 1s - loss: 2.1745 - wl_r: 0.3476 - wl_c: 1.8000 - wl_d: 0.0269 - l_r: 0.3476 - l_c: 1.8000 - l_d: 0.2689 - accuracy: 0.2852 - val_loss: 3.5766 - val_l_r: 0.5556 - val_l_c: 3.0007 - val_l_d: 0.2031 - val_accuracy: 0.0900 - 834ms/epoch - 26ms/step\n"
     ]
    }
   ],
   "source": [
    "ctrl.train_learner(epochs=10, verbose=2, proto_update_step_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation\n",
    "\n",
    "After the model has been trained, we can call multiple methods that will perform some kind of evaluation on the result. These methods begin with `eval_` followed by a description of the evaluation performed.\n",
    "\n",
    "First, we'll just call a simple method that plots the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl.eval_plot_loss_curves(which='both')\n",
    "ctrl.eval_plot_accuracy_curves(which='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the dataset we are training on is so small, overfitting is very likely and the results will not be great. The demo data is purely for demonstrating the usage of the API.\n",
    "\n",
    "Most of the evaluation methods in the API are geared toward the domain learner model. Examples of these methods are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a list defining the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the features of the encoded validation images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ctrl.eval_plot_scattered_features(\n",
    "    which='validation',\n",
    "    legend=legend\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain learner learns prototype points for each property/class in the learned domain. We can plot these prototypes in the space, as well as visualize the decoded prototype points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 322ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "ctrl.eval_plot_scattered_protos()\n",
    "ctrl.eval_show_decoded_protos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a single dimension of the latent space using the decoder, or visualize all dimensions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features...\n",
      "Fixed: [0.5783857326954603, -0.3200235665170476]\n",
      "Dimension: 1 - Min: -1.504 - Max: 2.125\n",
      "Dimension: 2 - Min: -1.867 - Max: 0.431\n"
     ]
    }
   ],
   "source": [
    "ctrl.eval_visualize_dimension(dimension=1)\n",
    "ctrl.eval_visualize_all_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semantic similarity function is used to help the domain learner obtain meaningful representations for the different properties in the domain space. We can plot a heatmap to visualize the pairwise similarities between property prototypes, and also plot similarity histograms using the encoded validation samples (the x-axis of each histogram has a range of 0 to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Plotting similarity histograms...\n",
      "Finished 100 of 100\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ctrl.eval_plot_similarity_heatmap(legend=legend)\n",
    "ctrl.eval_plot_similarity_histograms(legend=legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare true images to recovered images to assess the quality of the autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "ctrl.eval_compare_true_and_generated()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
